---
title: "Exam Business Analytics"
author: "Antonella Facini"
date: "2024-05-02"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## **About the Exam**

It will take 1.5 h Open Book

Theory: written part with theory and output interpretation (16 points)
Practice: on R, report on some analysis (18 points)

## **Structure**

1.  Time Series
2.  Quantile Regression
3.  Simulation and Monte Carlo Analysis

## **Forecasting and time series (1)**

```{r}
library(ggplot2, quietly = T)
library(dplyr, quietly = T)
```

The ability to forecast depends on:

-   how well we understand the factors that contribute to it;

-   how much data is available;

-   how similar the future is to the past;

-   variability of the process of interest;

-   whether the forecasts can affect the thing we are trying to
    forecast.

Some processes occur on a temporal scale (sales, temperature, stock
prices, number of new graduates in a country, etc) .

Even for processes that occur at continuous time scale, observations can
only take place at discrete time steps. Data is often collected at
regular time-step. Time is a defining property of the data.

For example, the evolution of atmospheric carbon dioxide dry air mole
fractions; monthly medicare Australia prescription data for antidiabetic
drugs; turnover for takeaway food services; Google stock prices at
closing time;

### Forecasting from a statistical perspective

Thing to be forecast: a random variable $y_t$; The distribution of the
forecast: If $I$ is all observations, then $y_t|I$ means “the random
variable $y_t$ given what we know in $I$”. The point forecast is the
mean (or median) of$y_t|I$ The forecast variance is $var_[y_t | I]$

A prediction interval or interval forecast is a range of values of $y_t$
with high probability.

With time series, $y_(t|t−1) = y_t |{y1, y2, . . . , yt−1}$.

$\hat{y}_(T+h|T) = E[y_T+h|y1, . . . , y_T ]$ (an h-step forecast taking
account of all observations up to time T).

### Time Series and Patterns

**Trend** : pattern exists when there is a long-term increase or
decrease in the data.

**Seasonality**: pattern exists when a series is influenced by seasonal
factors (e.g., the quarter of the year, the month, or day of the week).

**Cyclic**: pattern exists when data exhibit rises and falls that are
not of fixed period (duration usually of at least 2 years).

**Seasonality v Cyclic**: Seasonality usually has a constant/less
variable magnitude v. Cycle magnitude is more variable;

Seasonal pattern has constant length v. Cyclic pattern has variable
length;

Average length of cycle pattern is longer v. Average length of seasonal
pattern is shorter

Timing of peaks is circa predictable on the long term for seasonal data
v. Timing is unpredictable on the long term for cyclic data

Short term it is easier to predict potential peaks in cyclic data rather
that in seasonal data

### Autocorrelation and Lag Plot

Keep in mind that the purpose is to perform a **regression**

An important underlying assumption for the regression is **Variables
independence**

**In reality does this assumption hold?** No! Think of a time series of
ice-cream sales. All sales in August months are not independent one to
each other. Sales, temperatures and stock-prices of today sometimes are
influenced by yesterday values.

**Autocorrelation is the first tool used to detect either seasonality or
cyclicality**

We will compare a value at time $t$ with respect to a value at time
$t - k$ for different values of $k$, computing the correlation among the
value and the lagged one.

#### Lag Plot

A lag plot is a special type of scatter plot with the two variables
(X,Y) “lagged.” A “lag” is a fixed amount of passing time (K); One set
of observations in a time series is plotted (lagged) against a second,
later set of data.

```{r}
library(fpp3)

aus_production%>%
  filter(year(Quarter)>2000)%>%
  gg_lag(Beer, geom = 'point')
```

I notice seasonality:

-   **Quarters Clustering together**: if there's seasonality in the
    data, you'd expect to see patterns emerge where data points from the
    same quarter tend to be closer to each other. It means that the data
    points corresponding to the same quarter across different years tend
    to form distinct clusters or groups. This clustering indicates that
    there's a recurring pattern or seasonality within each quarter of
    the year.

-   **Potential Annual Seasonality**: At lag 4,there is a more specific
    pattern where not only the quarters cluster together, but there's
    also a linear pattern within each quarter across different years.
    This finding suggests an annual seasonality, meaning that there's a
    repeating pattern in beer production on a yearly basis.

#### Autocovariance and autocorrelation

**Autocovariance and autocorrelation**: measure linear relationship
between lagged values of a time series y.

**Note: when variables are independent then the correlation is null**

Sample autocovariance at lag $k$ is $c_k$

Sample autocorrelation at lag $k$ is $r_k$

Where:

$c_k = \frac{1}{T}\sum_{t=k+1}^{T} (y_t - \bar{y})(y_{t-k} - \bar{y})$

$r_k = c_k / c_0$

**The autocorrelation at lags 1,2,.. make up the autocorrelation
function or ACF**

*Australian Beer Production*

![Australian Beer Production, Time
Series](images/Screenshot%202024-05-02%20221959.png){width="664"}

```{r}
aus_production%>%
  filter(year(Quarter)>2000)%>%
  ACF()%>%
  autoplot()
```

**Annual seasonality confirmed, for each year the variables are strongly
linearly correlated**.

Specifically,

***Positive Autocorrelation at Lag 4, 8, 12, ...:*** there's a recurring
pattern where beer production tends to follow a similar trend each year

***Negative Autocorrelation at Lag 2, 6, 10, 14, ...:*** it reflects a
"reversal" or "alternating" pattern in the data. At these lags, we're
looking at pairs of quarters with specific time intervals between them:
6 months (lag 2), 1 year and 6 months (lag 6), 2 years and 6 months (lag
10), and so forth.

The negative autocorrelation at these lags suggests a pattern where the
quarters separated by these intervals exhibit an "inverse" relationship
in terms of beer production.

For example, if there's a peak in beer production during a particular
quarter, the corresponding quarter at the specified lag will tend to
have a decrease in production, and vice versa. This inverse relationship
can be due to various factors such as seasonal shifts, supply chain
dynamics, or consumer behavior patterns.

These negative autocorrelations at specific lags indicate a more complex
temporal pattern beyond simple yearly seasonality, suggesting additional
cyclical or alternating trends in the data.

*Retail Employment in the USA*

![Retail Employment, USA, Time
Series](images/Screenshot%202024-05-02%20222029.png){width="662"}

![Retail Employment, USA, ACF](images/retail_usa.png){width="664"}

By looking to the time series, it is evident an increasing trend, a
cyclical pattern and also a seasonal pattern.

By looking at the lag-plot it was evident that the increasing trend is a
much stronger component.

By looking at the ACF the strength of the increasing trend is confirmed:

***Decreasing Autocorrelation with Increasing Lags**:* In a time series
with a strong increasing trend, each observation tends to be larger than
the previous one. As a result, observations farther apart in time are
likely to be less correlated with each other

***Slower Decay in Autocorrelation**:* the decay in autocorrelation
might be slower compared to a stationary time series. This slower decay
occurs because the increasing trend introduces a systematic relationship
between observations that persists over longer time periods.

**BOTTOMLINE: When data have a trend (meaning it either increases or
decreases), the autocorrelations for small lags tend to be large and
positive (following the same path); When data are seasonal, the
autocorrelations will be larger at the seasonal lags (i.e., at multiples
of the seasonal frequency); When data are trended and seasonal, you see
a combination of these effects; Time series that show no autocorrelation
are called white noise.**

![Solved Exercise: 1B; 2A; 3D; 4C](images/ACF_recognition.png)\

### Transformations

Avoid comparing apples with oranges.

Think of comparison between absolute GDP of two countries. It is not
meaningfull. The value is influenced by the population. An adequate
transformation would be 'by population'.

-   Population adjustments (GDP of USA in 1960 and 2023), Calendar
    adjustments (total monthly sales,total yearly sale in leap years),
    Inflation adjustments

-   Mathematical adjustments: to stabilize the variance

#### Transformations: Log-Transformation

Sometimes the variability of the series changes with its level, think of
a time series with an increase in magnitude of the seasonal term.

Log's advantage: *interpretability*

When we take the logarithm of a variable, we're essentially transforming
the scale of the data.

When you take the logarithm of a variable and observe changes over time,
what you're essentially seeing are relative changes rather than absolute
changes.

Logarithmic transformation compresses large values more than small
values.

### Decomposition

Time series are characterized by: *Trend-cycle component (*$T_t$), a
*Seasonal component (*$S_t$), and a *Remainder Unexplained Component
(*$R_t$)

Data at a period t ($y_t$) can be then described as

$$y_t= f(S_t,T_t,R_t)$$

Composition of Seasonality can be either **Additive** or
**Multiplicative**

**Additive model**:appropriate if magnitude of seasonal fluctuations
does not vary with level.

**Multiplicative model**: appropriate if seasonal changes are
proportional to level of series; they can be transformed into an
additive model through log-adjustment.

$$y_t=log(S_t) - log(T_t) + log(R_t)$$ **Classical decomposition** can
be performed for additive and multiplicative seasonality.

### Moving Averages

**Purpose: to smooth out the seasonal component. It will be used a
moving average of order m established by the periodicity of the time
series.**

Moving averages by definition are symmetric and centered at time t, and
imply the need of an **odd** $m$.

When dealing with **even** $m$ take a moving average of the moving
averages.Therefore compute a **Weighted Moving Average**. For seasonal
data with even period m **take 2 × MA of period m (2 x m-MA)**.

```{r}
#i must make a weighted average, giving weights, specifically lower weights for outsiders
#MA4-> 5 observations taken, since mathematically you compute the moving average for two different sets;
#the weights are mathematically computed.

library(slider)

MAs<-aus_production%>%
  filter(year(Quarter)>2000)%>%
  select(Quarter,Beer)%>%
  mutate(MA5=slider::slide_dbl(Beer, .f = mean, .before = 2, .after = 2, .complete = TRUE), 
         MA4=slider::slide_dbl(Beer, .f = weighted.mean, .before = 2, .after = 2,w=c(1/8,1/4,1/4,1/4,1/8), .complete = TRUE))

ggplot(MAs%>% pivot_longer(cols = -Quarter))+
  geom_line(aes(Quarter, value, col=name))+
  theme_bw()
```

The greater m, the smoother the curve. The smaller m, the closer the
curve to the original series.

The greater m, the less observations we keep. The smaller m, the more
observations we keep.

Note: the choice of m affects drastically strongly seasonal data.
Obviously it reduces the seasonality component, the greater $m$.

### Classical Decomposition

We use the function 'classical decomposition'. It derives the trend
component, seasonal component, random component and also the adjusted
values for seasonality.

We first break down how to compute manually the seasonal term.

The trend component is derived as MA.

The seasonal component, in the model is computed as the seasonal mean of
detrended data

```{r}
#additive model
ausdecomp <- 
  aus_production %>% filter(year(Quarter) >2000) %>% 
    select(Quarter, Beer) %>% 
  model(classical_decomposition(Beer)) %>% 
  components()

#model's plot
ausdecomp %>% autoplot()

#seasonal term computation: derived from seasonal means: quanta varianza c'è in ogni stagione?
seasMeans <- ausdecomp %>% 
  mutate(q = quarter(Quarter), 
         # value - trend 
         detrended = Beer - trend) %>%
  as_tibble() %>% group_by(q) %>% 
  summarise(seasonal = mean(detrended, na.rm = TRUE)) %>%
  pull(seasonal) 

#just enforced to have 0 mean
seasMeans-mean(seasMeans)

#actual comparison to the original model
ausdecomp %>% slice(1:4) %>% pull(seasonal)
```

```{r}
#multiplicative model: seasonality increasing in magnitude
aus_production %>%
  autoplot(Gas)

#additive does not detect seasonality properly, look at the random component plot
AddGasDecomp<- aus_production%>%
  model(ngas_decomp=classical_decomposition(Gas, type='a'))%>%
  components()

#option 1: multiplicative
MultGasDecomp <- aus_production %>%
  model(gas_decomp= classical_decomposition(Gas, type = "m")) %>% 
  components() 

#option 2: log-transformation, additive
AddLGasDecomp <-  aus_production %>%
  model(lgas_decomp= classical_decomposition(log(Gas), type = "a")) %>% 
  components() 

AddGasDecomp%>%autoplot()
MultGasDecomp %>% autoplot()
AddLGasDecomp %>% autoplot()
```

**Classical decomposition drawbacks**: - The estimate of the
*trend-cycle* (also the remainder) is unavailable for the first and last
observations

-   The estimate of the *trend-cycle* tends to over-smooth rapid rises
    and falls (MA)

-   Assumption of constant seasonal terms might be too conservative

-   The estimate is not robust, very sensitive to large variations in
    small number of points

### Exponenatial Smoothing: forecasting

Main idea: forecasts are based on weighted averages of past
observations.

The weights are heavier for the more recent past.

The decay of the weights is exponential.

2 Cases: Data without trend/seasonal term; Data with trend/seasonal term

#### Simple Exponential Smoothing (SES)

**No trend; No seasonality**

1)  Data at time $h$ are equal to the last observed value of the series.

$$\hat{y}_{T+h|T} = y_T$$

-   the most recent observation is the only important one

-   previous observations provide no info

-   weighted average in which all the weight is given to the last
    observation

2)  Data at time $h$ are equal to the simple average of all observed
    data.

$$ \hat{y}_{T+h |T}= \frac{1}{T}\sum_{t=1}^{T}y_t$$

-   all observations are of equal importance

-   weighted average in which all observations have same weight

3)  SES: forecasted data are calculated using weighted averages, where
    the weights decrease exponentially as the observations come from
    further in the past;

$$\hat{y}_{T+1 | T}= \alpha y_T + \alpha (1 - \alpha)y_{T-1}+\alpha (1-\alpha^2)y_{T-2}... $$

-   where $0<\alpha<1$

-   $T+1$ forecast is a weighted average of all of the observations in
    the series;

-   The bigger alpha, the more importance is given to recent
    observations; The bigger alpha, the closer the forecasts to the real
    observations;

-   The smaller alpha, the more importance is given to observations from
    the more distant past; more importance to the overall mean; The
    smaller alpha, the closer the forecasts to the mean of observations;

```{r}
#TRY: plot the data just as it is; missing weekend values = steps in the graph
gafa_stock%>%
  filter(Symbol=='GOOG',year(Date)>=2018)%>%
  ggplot(aes(x=Date,y=Close))+
  geom_line()

#DATA PREPARATION; financial series: we must look at the trading day
goog <- gafa_stock %>%
  filter(Symbol == "GOOG", year(Date) >= 2018) %>%
  mutate(trading_day = row_number()) %>%
  update_tsibble(index = trading_day, regular = TRUE) 
goog_sub <- goog %>% filter(trading_day > 120) %>% 
  select(trading_day, Close) %>% 
  mutate(trading_day = trading_day-120)
goog_sub %>% autoplot(Close)
```

```{r}
#Fit SES model
fit<-goog_sub%>%
  model(SES=ETS(Close ~ trend('N')+ season('N')))

fit%>%glance()

#Visualize estimates: optimal alpha and l_0
fit%>%tidy

#Visualize the component form: Observation, Level, Remainder
fit%>%components()

#Plot the Original Time Series v Level Values
#NOTE. YOU ARE NOT FORECASTING ANY VALUES

fit%>%
  components()%>%
  ggplot()+
  geom_line(aes(x=trading_day, y=Close))+
  geom_line(aes(x=trading_day,y=level, col='red'))
```

```{r}
#compare different alphas: optimal v. small v. smaller
fit<-goog_sub%>%
  model(SES=ETS(Close~trend('N')+season('N')),
        SESa003=ETS(Close~trend('N', alpha=0.03)+season('N')),
        SESa007=ETS(Close~trend('N', alpha=0.07)+season('N')))

#components() gives observation and level, you would have to compute the fitted
#augment() creates dataframe, adds column .fitted, ideal for plots 
fit%>%augment()

#plot observations v. fitted values of the three models
fit%>%
  augment()%>%
  ggplot()+
  geom_line(aes(trading_day,Close))+
  geom_line(aes(trading_day,.fitted, col=.model))
        
```

**FORECASTING**

```{r}
#FORECASTING
fit%>%
  forecast(h=5)

#PLOT FORECASTED VALUES (NOTE:CANNOT PLOT DF, NO AUGMENTED)
fit%>%
  forecast(h=5)%>%
  autoplot(level=NULL)

#PLOT FORECASTED VALUES V. ORIGINAL TS
fit%>%
  forecast(h=50)%>%
  autoplot(goog_sub, level= NULL)

```

**Exponential Weighted Moving Average:**

-   it is the graph obtained from the SES model applied to all real
    observations;

-   starting point defined by $l_0$;

-   iterate the following formula until time $T$

At $t=1$, $\hat{y}_{1|0}= l_0$

At $t=2$, $\hat{y}_{2|1}= l_1 = \alpha y_1 + (1-\alpha) l_0$

At $t=3$, $\hat{y}_{3|2}= l_2 = \alpha y_2 + (1-\alpha) l_1$

...

At $t=T+1$
$$\hat{y}_{T+1|T}= l_T = \alpha y_T + (1-\alpha)l_{T-1} = \sum_{j=0}^{T-1} \alpha (1-\alpha)^j y_{T-j} + (1-\alpha)^T l_0 $$

**The component form of SES**

is given by:

*Forecast Equation* $$\hat{y}_{t+h|t}= l_t$$

*Smoothing Equation* $$l_t= \alpha y_t + (1-\alpha)l_{t-1}$$

-   where $l_t$ is the level or smoothed value of the time series at
    time $t$

-   by setting $h=1$ we get the fitted values of the time series

-   by setting $t=T$ we get the true forecasts beyond the training data

**Flat Forecasts**

All forecasts (no matter $h$) take the same value, equal to the last
level component. This is why it is only suitable if the time series has
no trend or seasonal component.

$$\hat{y}_{t+h|t}= l_t$$ for $h=1,2,3..$

**Parameters choice** : $\alpha$ and $l_0$ are estimated following the
minimizing SSE rule (just as in any regression)

#### Exponential Smoothing with Trend Component: Double Exp Sm or Holt's Method

**Trend description:**

-   consider the first obs and the last, then this would be the global
    description of the trend (either increasing or decreasing);

-   we could consider a local trend, so the change in $y_t$ and $y_t+1$.
    Locally, we can have both increases and decreases;

-   we will use a middle approach

**The average change is** : $\frac{1}{T}(y_T - y_1)$

**The component form of ES with Trend**

*Forecasting Equation*: $\hat{y}_{t+h|t}= l_t + h \cdot b_t$

*Level Equation*: $l_t =\alpha y_t + (1-\alpha)(l_{t-1}+b_{t-1})$

*Trend Equation*: \\(b_t= $\beta^*$(l_t -
l\_{t-1})+(1-$\beta^*$)b\_{t-1}\\)

-   $l_t$ is an estimate of the level of the series at time $t$; $b_t$
    is an estimate of the trend (slope) of the series at time $t$

-   $\alpha$ is the smoothing parameter for the level;$\beta^*$ is the
    smoothing parameter for the trend

-   *Level equation* is a weighted average of observation $y_t$ and the
    one step-ahead training forecast for time $t$ given by
    $l_{t-1}+b_{t-1} =\hat{y}_{t|t-1}$

-   *Trend equation* shows that $b_t$ is a weighted average of the
    estimated trend at time $t$ based on $(l_t - l_{t-1})$ and
    $b_{t-1}$, the previous estimate of the trend

-   The forecast will not be flat, instead it will be trending

-   The h-step-ahead forecast is equal to the last estimated level plus
    h times the last estimated trend value. Hence the forecasts are a
    linear function of h.

**Parameters choice**: optimal (\alpha, $\beta^*$, l_0, b_0) minimising
SSE

```{r}
#fit model: SES (flat forecast); DES (sloped forecast); dDES (dumped forecast;)
fit2<-goog_sub%>%
  model(SES=ETS(Close ~ trend('N')+ season('N')),
        DES=ETS(Close~ trend('A')+ season('N')),
        dDES=ETS(Close~ trend('Ad')+ season('N')))

#visualize observation, l_t, b_t and remainder
fit2%>%components()

#plot observed v. fitted
fit2%>%augment()%>%
  ggplot()+
  geom_line(aes(trading_day,Close))+
  geom_line(aes(trading_day,.fitted, col=.model))

#plot observation v. level : non penso abbia senso perche non consideri lo slope stupida
fit2%>%
  components()%>%
  filter(trading_day>100)%>%
  ggplot()+
  geom_line(aes(trading_day,Close))+
  geom_line(aes(trading_day,level, col=.model))

#forecast and plot: SES (same as before, straight line), DES (linear function of h), dDES
fit2%>%
  forecast(h=50)%>%
  autoplot(goog_sub, level=NULL)

#i guess i could try do this with aud prod knowing it won't work

```



#### Damped Exponential Smoothing with Trend Component (Damped Double Exp Sm)

The forecasts generated by Holt’s linear method display a constant trend (increasing or decreasing) indefinitely into the future.

This assumption is very strong.

It is introduced a parameter \(\phi\) that “dampens” the trend to a flat line some time in the future.

-  If \(\phi =1\), the model is a DES model (linear forecasting)

-  If \(0<\phi<1\), the model dampens the trend, so that it approaches a constant some time in the future;

-  *Short-time* forecasts are *trended* while *Long-Run* are *constant*

**The component form of Damped Exponential Smoothing**

*Forecasting Equation*: $\hat{y}_{t+h|t}= l_t + (\phi + \phi^1+ \phi^2+...\phi^h)\cdot b_t$

*Level Equation*: $l_t =\alpha y_t + (1-\alpha)(l_{t-1}+\phi b_{t-1})$

*Trend Equation*: \(b_t= $\beta^*$(l_t - l_{t-1})+(1-$\beta^*$)\phi b_{t-1}\)


#### Exponential Smoothing with Seasonal Component: Triple Exp Sm / Holt-Winter's model

-  comprises the forecast equation and three smoothing equations — one for the level \(l_t\), one for the trend \(b_t\), and one for the seasonal component \(s_t\)
 
-  smoothing parameters \(\alpha, \beta, \gamma\)

-  \(m\) is the frequency of the seasonality (monthly data \(m=12\), quarterly data \(m=4\))

-  two versions depending on the nature of the seasonal component. *Additive* when seasonal component is constant in magnitude. *Multiplicative* when seasonal component increases in magnitude with the level of the series.

##### Exp Sm with Additive Seasonal Component

**The component form of the Additive Method**

*Forecasting Equation*: $\hat{y}_{t+h|t}= l_t + h \cdot b_t + s_{t+h-m(k+1)}$ 

*Level Equation*: $l_t =\alpha (y_t - s_{t-m}) + (1-\alpha)(l_{t-1}+b_{t-1})$

*Trend Equation*: $b_t= \beta^*(l_t - l_{t-1})+(1-\beta^*)b_{t-1}$

*Seasonal Equation*: $s_t= \gamma (y_t - l_{t-1} - b_{t-1})+ (1-\gamma)s_{t-m}$

where:

-  $k$ is the integer part of $(h-1)/m$ 

-  Level equation shows a weighted average between the seasonally adjusted observation at time $t$ and the non-seasonal forecast for time $t$ ($\hat{y}_{t|t-1}= l_{t-1}+ b_{t-1}$)

-  Trend equation shows a weighted average between the actual trend and the estimated trend at time $t-1$ for time $t$

-  Seasonal equation is a weighted average between the current seasonal index and the seasonal index estimated for the same season last year ($m$ periods ago)

-  $\gamma = \gamma^*(1-\alpha)$

```{r}
#fit the model: triple exp sm additive
fit3<-aus_production%>%
  filter(year(Quarter)>2000)%>%
  select(Quarter, Beer)%>%
  model(TES=ETS(Beer ~ trend('A')+ season('A')+error('A')))

fit3%>%tidy()

fit3%>%components()

#plot observations v. fitted
fit3%>%
  augment()%>%
  ggplot()+
  geom_line(aes(Quarter, Beer))+
  geom_line(aes(Quarter, .fitted, col='red4'))

```
 
```{r}
#forecast and plot original ts with forecast
fit3%>%
  forecast(h=5)%>%
  autoplot(aus_production%>%filter(year(Quarter)>2000), level= NULL)

```
 
##### Exp Sm with Multiplicative Seasonal Component

**The component form of the Multiplicative Method**

*Forecasting Equation*: $\hat{y}_{t+h|t}= (l_t + h \cdot b_t)\cdot s_{t+h-m(k+1)}$ 

*Level Equation*: $l_t =\alpha \frac{y_t}{s_{t-m}} + (1-\alpha)(l_{t-1}+b_{t-1})$

*Trend Equation*: $b_t= \beta^*(l_t - l_{t-1})+(1-\beta^*)b_{t-1}$

*Seasonal Equation*: $s_t= \gamma \frac{y_t}{l_{t-1} - b_{t-1}}+ (1-\gamma)s_{t-m}$

```{r}
#ts with trend, seasonality increasing in magnitude
aus_production%>%
  select(Quarter, Gas)%>%
  ggplot()+
  geom_line(aes(Quarter,Gas))

#fit the multiplicative model 
fit4<-aus_production%>%
  select(Quarter, Gas)%>%
  model(TESm=ETS(Gas~trend('A')+season('M')+error('M')))

#plot the observations v. .fitted
fit4%>%
  augment()%>%
  ggplot()+
  geom_line(aes(Quarter, Gas))+
  geom_line(aes(Quarter, .fitted, colour = 'pink4'))

#forecast
fit4%>%
  forecast(h=70)%>%
  autoplot(aus_production%>%select(Quarter, Gas), level=NULL)

```


```{r}
#fit multiplicative v additive for a meant to be multiplicative
fit5<-aus_production%>%
  select(Quarter, Gas)%>%
  model(TESm=ETS(Gas~trend('A')+season('M')+error('M')),
        TESa=ETS(Gas~trend('A')+season('A')+error('A')))

#forecast: additive doesn't predict any increase in magnitude for seasonal component
fit5%>%
  forecast(h=70)%>%
  autoplot(aus_production%>%select(Quarter, Gas), level=NULL)
```


#### Damped Exponential Smoothing with Seasonal Component (Damped Triple Exp Sm)

##### Damped Triple Exp Sm Multiplicative

**The component form of the Multiplicative Method**

*Forecasting Equation*: $\hat{y}_{t+h|t}= [l_t + (\phi+\phi^1+\phi^2...\phi^h) \cdot b_t]\cdot s_{t+h-m(k+1)}$ 

*Level Equation*: $l_t =\alpha \frac{y_t}{s_{t-m}} + (1-\alpha)(l_{t-1}+\phi b_{t-1})$

*Trend Equation*: $b_t= \beta^*(l_t - l_{t-1})+(1-\beta^*)\phi b_{t-1}$

*Seasonal Equation*: $s_t= \gamma \frac{y_t}{l_{t-1} - \phi b_{t-1}}+ (1-\gamma)s_{t-m}$

```{r}
#fit the damped multiplicative model of Triple Exponential Smoothing
fit6<-aus_production%>%
  select(Quarter, Gas)%>%
  model(dTESm=ETS(Gas~trend('Ad')+season('M')+error('M')))

#plot original ts + forecasted
fit6%>%
  forecast(h=50)%>%
  autoplot(aus_production%>%select(Quarter, Gas), level=NULL)

```
```{r}
#comparison multiplicative v. damped multiplicative
fit7<-aus_production%>%
  select(Quarter, Gas)%>%
  model(dTESm=ETS(Gas~trend('Ad')+season('M')+error('M')),
        TESm=ETS(Gas~trend('A')+season('M')+error('M')))

fit7%>%
  forecast(h=70)%>%
  autoplot(aus_production%>%select(Quarter, Gas), level=NULL)
```
```{r}
#damped additive model of Triple Exponential Smoothing
lol<-aus_production%>%
  filter(year(Quarter)>2000)%>%
  select(Quarter, Beer)%>%
  model(TESa=ETS(Beer~trend('A')+season('A')+error('A')),
        dTESa=ETS(Beer~trend('Ad')+season('A')+error('A')))

lol%>%
  forecast(h=50)%>%
  autoplot(aus_production%>%filter(year(Quarter)>2000)%>%select(Quarter, Beer), level=NULL)
```
## Exercises

- Reconstruct the calculations needed to derive the components in the multiplicative decomposition that was obtained for the Australian Gas Production
- Compute a 5-MA and 9-MA for the oil prices in the `prices` dataset and overlay them to the original data ensuring that suitable labels are used to identify the different fits

```{r}
library(slider)

MAs<-prices%>%
  filter(year>1869)%>%
  select(year, oil)%>%
  mutate(MA5=slide_dbl(oil,.f=mean, .before=2,.after=2,.step=1, complete=TRUE),
         MA9=slide_dbl(oil,.f=mean, .before=4,.after=4,.step=1, complete=TRUE))

#non si legge la leggenda
MAs%>%ggplot()+
  geom_line(aes(year,MA5),col='red')+
  geom_line(aes(year,MA9), col='green4')+
  geom_line(aes(year,oil))+
  labs(title = '5-MA and 9-MA for oil prices',
       x='Year',
       y='Value')

#merge
MAs<-MAs%>%
  pivot_longer(cols = c(oil,MA5,MA9))

#plot
MAs%>%ggplot()+
  geom_line(aes(year, value, col=name))+
  labs(title = 'ce l ho fatta ehe')
```


- Decompose the US Retail sector time series

```{r}

#us_retail <- 
  #us_employment %>%
 #   filter(Title == "Retail Trade", year(Month) >= 1980) 

#us_retail%>%ggplot()+
  #geom_line(aes(Month,Employed))

#dec<-us_retail%>%
 # select(Month, Employed)%>%
  #classical_decomposition(Employed, type='a')
''

```

Use an additive classical decomposition. Plot the components and comment on possible issues in the resulting decomposition. 

- Derive the numerical values for the multiplicative triple exponential smoothing ("Triple_Mult") of the Australian beer production model seen above

- Focus on the Tobacco production in Australia: the values are contained in the `aus_production` dataset: discuss whether a single, double or triple exponential smoothing would be more appropriate to smooth this time series. Compare the 20-step ahead forecast obtained when using a triple exponential smoothing with a damped and non-damped trend. 

- Focus on the `global_economy` dataset: pick a country (say France) and produce a simple exponential smoothing forecast for the country GDP. Discuss whether a double exponential smoothing or triple exponential smoothing would be suitable to use for smoothing the time series. 

## **Quantile Regression**

**Quantile definition:** \(Q_{(p)}= P(Y <Q_{(p)})>p\)

Model any quantile as a function of predictive variables: $Q_{(p)}|X=x$

Model a particular quantile $(Q(p))$ of a distribution, given certain values for the covariates $(X = x)$

**Quantiles depiction:**

-  They are strictly related to the CDF (cumulative distribution function); By looking at the y-axes (cumulative probability), quantiles are easily identifiable

-  Quantile function: x-axes depicts cumulative probability; y-axes depicts the corresponding value for the observed variable ($Q_{(p)}$)

**Empirical Cumulative Distribution Function**: estimator, it is an empirical version of the CDF. For an iid sample it is defined as $\hat{F}_t=\frac{number of observations < t}{n}$

**Sample Quantiles**: sample quantiles are estimated by inverting the ecdf. Then, for a given $p$ estimate $Q(p)$

**Sampling Distribution of Sample Quantiles**: boh

**Quantile as a Minimization Problem**: 

F.e. Consider *the median* (the value that divides in two equal parts the dataset). I want to find a $c$ which has on average the lowest absolute distance from the rest of the dataset.

\[Med = argmin E[|Y-c|]\]

More generally we can define any quantile as a minimization problem, where:

\[Q(p)= argmin(d_p(Y,c))\]
and \[dp(Y, c) =
\left\{
\begin{cases}
    (1 - p) |Y - c| & \text{for } Y < c \\
    p |Y - c| & \text{for } Y \geq c
\end{cases}
\right.\]

**Quantile based Summary statistics**:

-  Scale can be summarised with a quantile-based scale measure (QSC) at a selected $p$:
\(QSC(p)= Q(1 - p) - Q(p)\) for $p \le 0.5$

IQR is obviously QSC(0.25)

-  Skewness can be summarised with a quantile-based skewness or assymetry index

\(QSK= \frac{Q(1-p)-Q(0.5)}{Q(0.5)-Q(p)}-1\) for $p \le 0.5$

Obviously, if the distance is not the same than we have skewd data. $QSK >0$ we have right skewness.

```{r}
#plot data for income; distribution of income: rightly skewed
incomeEx <- read.table('C:/Users/anton/OneDrive/SKOOL/R/BUSINESS ANALYTICS/DATA/incomeSurvey.csv', header=TRUE,sep=",")
incomeEx$income <- incomeEx$cinc/1000
p1 <- ggplot(incomeEx) +
  geom_histogram(aes(income), col  = "darkred", fill="white") + 
  labs(x = "Income", title = "Income (100s USD)")
p1

#right skewness: median < mean
mean(incomeEx$income)
median(incomeEx$income)

#make it visible
p2 <- ggplot(incomeEx) +
  geom_histogram(aes(income), col  = "darkred", fill="white") + 
  labs(x = "Income", title = "Income (100s USD)")+
  geom_vline(xintercept =mean(incomeEx$income), col='pink')+
  geom_vline(xintercept= median(incomeEx$incoem), col='blue')
p2
```

Compute quantiles for the data:
```{r}
#compute quantiles
pquant<-quantile(incomeEx$income, probs=c(0.1,0.25,0.5,0.75,0.9))

#plot income distribution + quantiles
ggplot()+
  geom_histogram(aes(incomeEx$income), col='darkred',  fill='white')+
  geom_vline(xintercept=pquant)
```

**Note**:

-  The distance of 75 quantile from the median is much larger than the distance of the 25q from the median. Indication for assymetry in the distribution of the data;

-  We could say that the 25% of the sample earns `r pquant[2]` or below,meaning this income is exceeded by 75% of the surveyed households; we could say that only 10% of the sample earns above `r pquant[5]`;


Specifically we can also compute the $QSK(.25)$ and $QSK(.1)$. Both these values are positive, indicating right skewness.

```{r}
#assymetry? yes
#75q - 50q; 50q - 25q
pquant[4]- pquant[3]; pquant[3] - pquant[2]  

#90q - 50q; 50q - 10q
pquant[5]- pquant[3]; pquant[3] - pquant[1]

#assymetry index:QSK(0.25), QSK(0.1)
qsk_25<- (pquant[4]- pquant[3])/(pquant[3] - pquant[2])-1

qsk_01<-(pquant[5]- pquant[3])/(pquant[3] - pquant[1])-1 
```

**Quantiles for known distribution**

```{r}
df<-data.frame(x=seq(0,450))

ggplot(df)+
  geom_line(aes(x=x, y=dnorm(x, mean=47.5,sd= 34.2)))+
  geom_line(aes(x=x, y=dgamma(x, 1.9, rate=0.04)), col='green')
```

We compare the values of several key quantiles: notice that the gamma is fairly asymmetric (right-skewed), so $Q(.75) - Q(.50) \neq Q(.50)-Q(.25)$ and $Q(.90) - Q(.50) > Q(.50)-Q(.1)$

```{r}
pn <- qnorm(c(0.1,0.25,0.5,0.75,0.9), 47, 34.2)
pg <- qgamma(c(0.1,0.25,0.5,0.75,0.9), 1.9, rate = 0.04)
pn[4] - pn[3]; pn[3] - pn[2] #obviously same measures since it is normal
pn[5] - pn[3]; pn[3] - pn[1]
pg[4] - pg[3]; pg[3] - pg[2] #obviously different measures since no normal
pg[5] - pg[3]; pg[3] - pg[1]
```

Compute the QSK(0.1) value for the two distributions:
```{r}
(pn[5]-pn[3])/(pn[3]-pn[1]) - 1 ## symmetric  
(pg[5]-pg[3])/(pg[3]-pg[1]) - 1 ## right skewed 
```

### The Linear Quantile Regression Model

#### Definition

\[Q^\tau(Y|X)= \beta_{0}^\tau + \beta_{1}^\tau X\]

-  \(Q_{\tau}\) of Y is a value, which cumulates a \(\tau\) proportion of the dataset;

-  \(Q_{\tau}(Y|X)\) is the conditional quantile; it must be an estimated line, since we are interested in the value representing the $\tau$ quantile **given** the value of another variable. The $\tau$ quantile probably changes as the predictive variable changes, so we estimate a line ($\beta_0$ and $\beta_1$) to describe these changes on conditional distribution of Y given X for the $\tau$ quantile

#### Estimation

In general QRM estimates for quantile \(\tau\) minimise:

\[X_n = \sum_{i=1}^{n} d\tau (y_i, a + b x_i)\]

where 
\[d\tau (y_i, a + b x_i) = \begin{cases} 
(1 - \tau) |y_i - a - b x_i| & \text{for } y_i < a + b x_i \\
\tau |y_i - a - b x_i| & \text{for } y_i \geq a + b x_i 
\end{cases}\]

```{r}
library(dplyr)
library(broom)
library(ggplot2)
theme_set(theme_bw())
library(quantreg)
```

```{r}
#load data
data(engel)

#histogram of foodexp
ggplot(data=engel)+
  geom_histogram(aes(foodexp))

#scatter plot of the data
ggplot(engel)+
  geom_point(aes(income, foodexp))+
  labs(x='Annual Income per Household',
       y='Food Expenditure',
       title= 'Food Expenditure per level of income')
```

To plot the quantile regression we can either use:

-  'geom_quantile', which automatically computes the relevant quartiles (you can also specify quantiles with a vector of probabilities)

-  compute the model using function 'rq'. Plot the model using 'ggplot'  and the 'augment' function on the model. add a layer for the line, specifying fitted values grouped by tau as y-values

-  compute the model using function 'rq'. Plot the model using 'ggplot' and the function 'tidy'. Keep in mind you probably need to relaborate the data.frame (wider_pivot). add a layer for the line which takes in slope and intercept ('geom_abline')

```{r}
#comparison median regression v. linear regression v. linear reg without outliers
ggplot(engel, aes(income, foodexp))+
  geom_point()+
  geom_quantile(quantiles = 0.5, col='pink4')+
  geom_smooth(method='lm', se=FALSE)+
  geom_smooth(data=engel%>%filter(income<4000), method = 'lm', col='red')
```

**Note**: lm is highly affected by outliers; by removing 1 observation, new model is much more similar to q_median_reg

```{r}
#quantile reg on relevant quartiles (geom_quantile)
ggplot(engel, aes(income, foodexp))+
  geom_quantile()+
  geom_point()

#specify quantiles
ggplot(engel, aes(income, foodexp))+
  geom_quantile(quantiles= c(0.1,0.25,0.5,0.75,0.9))+
  geom_point()
```
**Note**:

-  There seems to be an indication that the variability of the distribution is increasing: the range between the *p*th and *(1-p)*th quantile is increasing for various values of *p*. 
-  The variability we see for food expenditure for lower levels of income is much lower than the variability in food expenditure for higher levels of income. 

-  There is an indication that families with higher incomes spend more on food expenditure, but for the strength of the relationship becomes steeper for higher quantiles.  it means that as you move along the income axis, the corresponding increase in food expenditure becomes more pronounced. steeper strength= the rate at which food expenditure increases for wealthier families is higher compared to less wealthy families.

**Model**

Until now we plotted a quantile regression line. Now we develop the model.

```{r}
#create model; by default a median regression
rq_engel<-rq(foodexp~income, data = engel)
```

Access information on the model

```{r}
#coefficinets, lowe bound, upper bound
summary(rq_engel)

#bootstraped-based standard deviation
summary(rq_engel, se='boot')

#estimate, lower bound, upper bound, tau; you can also bootstrap
#Intercept:It is the predicted median food expenditure when all predictor variables are zero
#Income:The estimate for income represents the estimated change in the median food expenditure for a one-unit increase in income, holding all other variables constant
rq_engel%>%tidy()

rq_engel%>%tidy(se='boot')

#glance()
rq_engel%>%glance()

#augment() -> x,y, fitted, tau and res; suitable for plots
rq_engel%>%augment()

#example for augment + ggplot

ggplot(data=rq_engel%>%augment())+
  geom_point(aes(income, foodexp), col='grey')+
  geom_line(aes(income,.fitted), col='red')
```


We could also want to estimate a quantile regression model for several quantiles: this can be done with a unique call using the option `tau`: 
```{r}
mult_rq_engel<-rq(foodexp~income, data=engel, tau=c(0.1,0.25,0.5,0.75,0.9))

mult_rq_engel%>%tidy()

mult_rq_engel%>%augment()
```

Second and third option for plotting quantile regression:
```{r}
library(tidyr)

# 2) plot multiple qr: use estimated intercepts/slopes (geom_abline)
#opposite of pivot_longer
widedf <- mult_rq_engel %>% tidy(se.type = "boot") %>% 
  select(term, estimate, tau) %>% 
  pivot_wider(names_from = term, values_from = estimate)

ggplot() + 
  geom_point(data = engel, aes(income, foodexp)) + 
  geom_abline(data = widedf, aes(slope = income, 
                                 intercept = `(Intercept)`), 
              col ="deeppink3")  + ylim(0,4000)

#3) plot multiple qr: augment() and group by tau 
aug1 <- mult_rq_engel %>% augment()
ggplot() + 
  geom_point(data = engel, aes(income, foodexp)) + 
  geom_line(data = aug1, aes(income, .fitted, group = .tau), 
            col = "dodgerblue")  + ylim(0,4000)


mult_rq_engel %>% augment(newdata = data.frame(income = c(3000, 8000)))


#plot slopes as a function of tau; slopes increase as tau increases
mult_rq_engel%>%
  tidy(se='boot')%>%
  select(term,tau, estimate)%>%
  pivot_wider(names_from=term,values_from =estimate )%>%
  ggplot(aes(tau, income))+
  geom_line()+
  geom_point()+
  labs(title= 'Slopes of Quantile Regression')
  
```
Let's now see what the predicted values look like:
```{r}
#1)create new dataset for predictions  
pred_df <- tibble(income = seq(200, 5000, by = 200))

#use augment(newdata=); apply model to new data
pred_df <- mult_rq_engel%>% augment(newdata = pred_df)
#order data; plot predicted 
ggplot(pred_df[order(pred_df$income),]) + 
  geom_line(aes(income, .fitted, col = .tau)) + 
  labs(title = "Engel data - prediction for several quantiles") +
  theme(legend.position = c(0.05,0.95), legend.justification = c(0,1))


#2) model applied for specific values of the income variable
mult_rq_engel %>% augment(newdata = tibble(income = c(3500, 4000)))

ggplot()+
  geom_line(data= mult_rq_engel%>%augment(newdata=tibble(income=c(3500,4000))),
            aes(income, .fitted, col=.tau))
```

**Equality of slopes testing**: test (making inference) on whether the slopes  for the quantile regression on different $\tau$s are the same. If that is the case the difference on the regression lines would be a constant and we could assume conditionally identically distributed response variable.

\[H_0: \beta_{1}^{(\tau_1)} = \ldots = \beta_{1}^{(\tau_q)} \quad \text{against}
\quad H_1: \beta_{1}^{(\tau_1)}  \neq \ldots \neq \beta_{1}^{(\tau_d)}\] 

```{r}
anova(mult_rq_engel)
```
The F-value is statistically significant meaning we can reject the null hypothesis of equality of slopes. This is coherent with what we observed before, slopes changed as $\tau$ changed.

### The quantile regression with multiple explanatory variables

**Definition**: 

\[Q_{\tau}(Y|X_1,...X_d)= \beta^{\tau}_0+\beta^{\tau}_1x_1i.....+\beta^{\tau}_dx_di+\epsilon^{\tau}_j\]

where:

-  X is a matrix of $d+1$ columns and as many rows as the observations of the dataset

-  $\beta^{\tau}$ is a vector containing as $d+1$ estimates (not $d$ because of the intercept $\beta_0$)

-  Interpretation of $\beta_j$: the effect of $x_j$ on the quantile $\tau$ as the effect of other variables is considered;


**Estimation of the model**

Does the addition of $X_j$ improve the goodness of fit? Are they relevant?

How can we make inference? **Bootstrap**


```{r}
rents<-read.table("C:/Users/anton/OneDrive/SKOOL/R/BUSINESS ANALYTICS/DATA/rent99.raw", header=TRUE)

ggplot(rents) + geom_point(aes(area, rent)) #strong increase in rent as area increases
ggplot(rents) + geom_point(aes(yearc, rent)) #not so evident increasing trend over time
```

Dataset:
-  rent: net rent per month (euros)

-  rentsqm: net rent per month  per square meter (euros)

-  area Living area (square meters)

-  year: year of construction

-  location: as judged by an expert coded as 1=average location; 2: good 
location; 3= top location 

-  bath: quality of the bathroom 0: standard; 1: premium

-  kitchen: quality of the kitchen 0: standard; 1: premium

-  cheating: presence of central heating 0: without; 1: with
district: district in Munich


```{r}
#include area and year as predictors for rent
library(quantreg)
rqfit_munich <- rq(rent ~ area+yearc, 
                   data = rents, 
                   tau = c(0.1,0.25,0.5,0.75,0.9))
#visualize model
summary(rqfit_munich)

#wrong plot: we have 2 explanatory variables
pred_df <- rqfit_munich %>% 
  augment()

ggplot(pred_df)+
  geom_point(aes(area,rent), col='grey')+
  geom_line(aes(area, .fitted,col=.tau))+
  labs(title = 'Multi-Exp Quantile Reg',
       xlab='Area (sqmt)',
       ylab='Rent (euros)')

#SOLUTION: FIX THE VALUE FOR ONE OF THE EXPLANATORY VARIABLES
#create a tibble with the values of area (summary(rents$area), get min and max),
#and then fix the value for year (summary(rents$yearc) look for the median)


#plot rent v. area (fixed year)
pred_df<-rqfit_munich%>%
  augment(newdata=tibble(area=seq(20,160, by=5), yearc=1959))

ggplot(data=pred_df)+
  geom_point( data=rents,aes(area, rent), col='grey')+
  geom_line(aes(area, .fitted, col=.tau ))+
  labs(title = 'Rents and Area',
       subtitle = 'Yearc is 1959')

#questa è la regressione quantile multipla per i quantili rilevanti, con variabile predittive anno e area, 
#e la rappresentazione del modello 

```


```{r}
#fix area and plot rents v. year (fixed area)

summary(rents$area)
summary(rents$yearc)

pred_df<-rqfit_munich%>%
  augment(newdata=tibble(area=65, yearc=seq(1918,1997, by=1)))

ggplot()+
  geom_point(data=rents, aes(yearc, rent), col='grey')+
  geom_line(data=pred_df, aes(yearc, .fitted, col= .tau))
```

Let's look at the residuals:

```{r}
#plot residuals (recall residuals = obs- fitted) ;
library(broom)

#area: per tutti i quantili all'aumentare dell'area, la variabilità dei residuals aumenta
rqfit_munich %>% augment() %>% 
  ggplot(aes(area, .resid)) + geom_point() + 
  facet_grid(~.tau)

#year
rqfit_munich %>% augment() %>% 
  ggplot(aes(yearc, .resid)) + geom_point() + 
  facet_grid(~.tau)



  
```

Interpretation fo residuals plot:

-  increasing area, increasing variability of residuals for each quantile $\tau$

-  the residuals show some indication that the assumption of a linear relationship between `area` and `rent` for all quantiles might be not sufficient to capture the complex structure of the relationship between area and rent for all quantiles (particularly the more extreme ones)



**More complex model fit**

```{r}
rqfit2_munich <- rq(rent ~ area+yearc+I(area^2)+I(yearc^2), 
                   data = rents, 
                   tau = c(0.1,0.25,0.5,0.75,0.9))

#visualize dataset
rqfit2_munich%>%augment()

summary(rqfit2_munich, se.type ='boot')
```

It seems that the added coefficients are significant for some of the quantiles.

**Comparison between two models**
We can also compare the two nested models using `anova` but the function only work comparing nested models for one quantile at the time. 

Let's take the models for $\tau = 0.1$ and $\tau = 0.9$. We have two models: 

* a more complex model $rent_i = \beta^{(\tau)}_0 + \beta^{(\tau)}_1 area_i + \beta^{(\tau)}_2 yearc_i + \beta^{(\tau)}_3 area^2_i + \beta^{(\tau)}_4 yearc_i^2+ \varepsilon_i^{(\tau)}$
* a simpler, nested, model $rent_i = \beta^{(\tau)}_0 + \beta^{(\tau)}_1 area_i + \beta^{(\tau)}_2 yearc_i + \varepsilon_i^{(\tau)}$ 

```{r}
#just write the models to be compared; must be nested; complex v. simple

#tau=0.1
anova(rq(rent~area+yearc, data=rents, tau=0.1),
      rq(rent~area+yearc+I(area^2)+I(yearc^2), data=rents, tau=0.1))

#we can reject the null hypothesis of beta_3 and beta_4=0
#we can assume that for tau=0.1 (0.1 quantile) the complex model is more suitable
#keeping the two quadratic terms could be useful for several quantiles

#tau=0.5: same result
anova(rq(rent~area+yearc, data=rents, tau=0.5),
      rq(rent~area+yearc+I(area^2)+I(yearc^2), data=rents, tau=0.5))

#tau=0.9: same result
anova(rq(rent~area+yearc, data=rents, tau=0.9),
      rq(rent~area+yearc+I(area^2)+I(yearc^2), data=rents, tau=0.9))

```

**Equality of Slopes test**

Use 'anova' on the second model fit; Differently from before the predictive variables are multiple, 'anova' function evaluates equality of all slopes

In order to evalute the slopes of each predictor individually, we must set 'joint=FALSE'
```{r}
anova(rqfit2_munich, joint=FALSE)
```

For all predictors, we reject the null hypothesis of slope estimates equality (among all $\tau$s). Therefore, there is an indication for a change in distribution  in 'rents' for different levels of 'year' and 'area'.


Let's visualize the predicted quantiles - and let's do so by looking at the effect of area on rent for different fixed values of `yearc`. We wish to show this in a faceted plot, so we will make use of the `expand_grid` function from the `tidyr` package.

```{r}
pred_df <- rqfit2_munich %>% 
  augment(newdata = expand_grid(area = seq(20,160,by = 5), yearc = c(1918, 1959, 1984))) %>% 
  mutate(facyearcc = paste("Year is", yearc))

ggplot(pred_df) + 
  geom_point(data = rents, aes(area, rent), col = "grey80") + 
  geom_line(aes(x=area, .fitted, col = .tau)) + 
  facet_grid(~factor(facyearcc)) + 
  labs(title = "Rent Price (euro) and year of construction - Prediction", 
       subtitle = "Munich rent data",
       y = "Rent (euros)", x = "Area (square meters)")
```

Let's do the opposite, let's look at hte effect of the 'year' on rent for different fixed values of the 'area'.

```{r}
pred_df <- rqfit2_munich %>% 
  augment(newdata = expand_grid(area = c(37,65,98), yearc = seq(1918,1997,by = 2))) %>% 
  mutate(facarea = paste("Area is", area))

ggplot(pred_df) + 
  geom_point(data = rents, aes(yearc, rent), col = "grey80") + 
  geom_line(aes(x=yearc, .fitted, col = .tau)) + 
  facet_grid(~factor(facarea)) + 
  labs(title = "Rent Price (euro) and year of construction - Prediction", 
       subtitle = "Munich rent data",
       y = "Rent (euros)", x = "Year")

#The variability of the data is much higher when area is larger
```


### Quantile regression - changes in the distribution properties

Once we have the predicted values for different quantiles we can also plot some measure of Scale and Shape as a function of some explanatory variable to assess whether this is changing. 

We had defined

\begin{equation*}
  \begin{array}{ll}
  QSC(p|X) & = Q(1-p|X) - Q(p|X) = (\beta_0^{(1-p)}+\beta_1^{(1-p)}X) - (\beta_0^{(p)}+\beta_1^{(p)}X) =\\
 & = (\beta_0^{(1-p)} - \beta_0^{(p)}) + (\beta_1^{(1-p)} - \beta_1^{(p)})X    \end{array}
\end{equation*} 


\begin{equation*}
  \begin{array}{ll}
QSK(p|X) & = \frac{ Q(1-p|X) - Q(0.5|X)}{Q(0.5|X) - Q(1|X)}-1 = \frac{\beta_0^{(1-p)}+\beta_1^{(1-p)}X - \beta_0^{(0.5)}-\beta_1^{(0.5)}X}{\beta_0^{(0.5)}+\beta_1^{(0.5)}X - \beta_0^{(p)}-\beta_1^{(p)}X} -1  =  \\
  & = \frac{(\beta_0^{(1-p)} - \beta_0^{(0.5)}) +(\beta_1^{(1-p)}-\beta_1^{(0.5)})X}{(\beta_0^{(0.5)} - \beta_0^{(p)}) +(\beta_1^{(0.5)}-\beta_1^{(p)})X} -1
    \end{array}
\end{equation*} 

**If the scale is the same $\beta_{1}^{(1-p)}=\beta_{1}^{(p)}$ and QSC(p|X) reduces to a constant.**

**Under symmetry $\beta^{1-p}_1-\beta_{1}^{0.5}=\beta_{1}^{0.5}-\beta^{p}_1$ and $\beta^{1-p}_0-\beta_{0}^{0.5}=\beta_{0}^{0.5}-\beta^{p}_0$ therefore QSK(p|X) reduces to 0.**

We compute $QSC(0.25)$ and $QSC(0.9)$: we do that for fixed values of `yearc` and allowing `area` to vary: 
```{r}
library(tidyr)
pred_df <- rqfit2_munich %>% 
  augment(newdata = expand_grid(area = seq(20,160,by = 5), yearc = c(1918, 1959, 1984))) %>% 
  mutate(taulevel = paste0("tau",.tau)) %>% select(-.tau) %>% 
  pivot_wider(names_from = taulevel, values_from = .fitted)
pred_df <- pred_df %>% mutate(QSC25 = tau0.75 - tau0.25,
                              QSK10 = (tau0.9 - tau0.5)/(tau0.5 - tau0.1)-1)
```

```{r}
#vedo per esempio che la scale nel 1918 per un income di 40 è 200, mentre per un income di 140 è 500. se la scale aumenta significa che la variabilità è aumentata. inoltre la starting scale (starting variability) nel 1918 era più alta. (heteroskedasticity)

#Scale increases for increasing values of area, but appears to bit lower in 1959. 
ggplot(pred_df) + 
  geom_line(aes(area,QSC25)) +facet_grid(~factor(yearc))
```

```{r}
#Skewness has a complex way of changing and it interacts with `yearc`.

ggplot(pred_df) + 
  geom_line(aes(area,QSK10)) +
  geom_hline(yintercept = 0, col='red')+
  facet_grid(~factor(yearc))
```


## Exercises

1. Obtain the dataset `bodyfat` in which the weight and the height of a random sample of 255 men are stored: 

```{r,eval=FALSE}
bodyfat <- read.table("C:/Users/anton/OneDrive/SKOOL/R/BUSINESS ANALYTICS/DATA/bodyfat.txt", header=TRUE)
bodyfat <- bodyfat[,c("Weight","Height")]
```

We wish to construct a regression model in which the Weight of a man is regressed against its Height. 
Use a quantile regression framework to answer (among others) the following questions: 

* What is the effect of Weight on Height: does this appear to change for different quantiles? Compute the estimated coefficients and provide a visual display. 
```{r}

#rq_bodyfat<-rq(Height~Weight, data = bodyfat, tau = c(0.1,0.25,0.5,0.75,0.9))

#coefficients
#rq_bodyfat%>%tidy()

#1 visual display
#ggplot(bodyfat)+
 # geom_point(aes(Weight, Height),col='grey')+
  #geom_quantile(aes(Weight, Height), quantiles=c(0.1,0.25,0.5,0.75,0.9))

#2 visual display
#ggplot()+
 # geom_point(data=bodyfat, aes(Weight, Height), col='grey')+
  #geom_line(data=rq_bodyfat%>%augment(), aes(Weight, .fitted, group=.tau), col='green4')

#is there evidence for change in the effect on different quantiles? no

#p-value >alpha(0.05)
#anova(rq_bodyfat)

#visualization confidence intervals for estimates, different taus;
#plot is consistent with anova test
#rq_bodyfat%>%tidy()%>%filter(term =='Weight')%>%
 # ggplot()+
  #geom_point(aes(tau, estimate), col='grey')+
  #geom_line(aes(tau, estimate), col='lightgrey')+
  #geom_segment(aes(x=tau,xend=tau,
   #            y=conf.low, yend=conf.high))

#It appears that there could be a positive relationship between Weight and Height and there might be  some variability, especially for higher values of weight.

#Nevertheless by carrying an ANOVA test for equality of slopes, there isn't enough evidence to suggest that the slopes are significantly different from each other.

#NO:There is an indication that males with higher weight will display higher height, and the strength of the relationship becomes steeper for higher quantiles.  it means that as you move along the weight axis, the corresponding increase in height becomes more pronounced.

#There seems to be an indication that the variability of the distribution is increasing: the range between the p-th and (1-p)th quantile is increasing for various values of p, especially for p=0.25.

```


* Compute the 95\% confidence interval for the slope parameter of the median regression. Also indicate whether there is evidence that its value is null.
```{r}
#estimate and confidence interval for slope parameter tau=0.5
#rq_bodyfat%>%tidy()%>%filter(tau==0.5, term=='Weight')

#We are 95% confident that the true value of tau=0.5 lies between 0.03996280 and	0.06845537. 
#no evidence for nullity of slope  for median regression.

```



* Produce some plots which explore whether the scale or shape of the Weight distribution change as a function of Height. 
```{r}
#summary(bodyfat$Weight)
#summary(bodyfat$Height)

#new fitted dataset; mathematical computation
#index_rq<-rq_bodyfat%>%
#  augment(newdata=expand_grid(Weight=seq(119,262,by=1), Height=seq(64,77, by=1)))%>%
#  mutate(taulevel=paste0('tau',.tau))%>%
 # select(-.tau)%>%
#  pivot_wider(names_from=taulevel, values_from = .fitted)%>%
#  mutate(QSC_025=tau0.75-tau0.25, 
 #        QSC_01=tau0.9-tau0.1, 
  #       QSK_01=(tau0.9-tau0.5)/(tau0.5-tau0.1)-1,
 #        QSK_025=(tau0.75-tau0.5)/(tau0.5-tau0.25)-1)

#scale: QSC(0.25) conditional to levels of weight
#ggplot(index_rq)+
 # geom_line(aes(Weight,QSC_025))

#scale: QSC(0.1) conditional to levels of weight
#ggplot(index_rq)+
 # geom_line(aes(Weight,QSC_01))
#shape: QSK(0.25) conditional to levels of weight
#ggplot(index_rq)+
 # geom_line(aes(Weight,QSK_025))+
 # geom_hline(yintercept=0, col='red')

#shape: QSK(0.1) conditional to levels of weight
#ggplot(index_rq)+
#  geom_line(aes(Weight,QSK_01))+
#  geom_hline(yintercept=0, col='red')

```



* Is there strong evidence against the distribution of Weight to be different for different levels of Height? 

2. Carry out a single predictor quantile regression analysis for the Munich rents dataset investigating the effect of `area` on `rent` and on `rentsq`. Answer questions similar to those presented in Exercise 1. 
```{r}
rents<-read.table("C:/Users/anton/OneDrive/SKOOL/R/BUSINESS ANALYTICS/DATA/rent99.raw", header=TRUE)
```

```{r}
#model rent~area
area_rqrents<- rq(data=rents, rent~area, tau=c(0.1,0.25,0.5,0.75,0.9))

#model rentsqm~area
area_rqrentsqm<- rq(data=rents, rentsqm~area, tau=c(0.1,0.25,0.5,0.75,0.9))
```

```{r}
#model rent~area
#display data model rent~area
ggplot()+
  geom_point(data=rents, aes(area, rent), col='grey')+
  geom_line(data=area_rqrents%>%augment(), aes(area, .fitted, group = .tau), col='green')

#coefficients
area_rqrents%>%tidy()

#is there evidence for change in the effect on different quantiles? yes
anova(area_rqrents)

area_rqrents%>%
  tidy()%>%
  filter(term=='area')%>%
  ggplot()+
  geom_point(aes(tau,estimate))+
  geom_line(aes(tau, estimate), col='lightgrey')+
  geom_segment(aes(x=tau, xend=tau,
                   y=conf.low, yend=conf.high))
  
#both the anova test and the plot on the confidence intervals regarding the estimates
#indicate that we can reject the hypothesis of equality of slopes for different taus

#on average the rent increases as the area increases; the effect of area changes on different quantiles,
#the relationship becomes stronger for higher quantiles, the slope is steeper; apartments with higher
#area will pay higher rent

#There seems to be an indication that the variability of the distribution is increasing: the range between the *p*th and #*(1-p)*th quantile is increasing for various values of *p*. the variability we see for rent for lower levels of area is much lower than the variability in rent for higher levels of area. This will be tested with QSC and QSK 

```

```{r}
#Compute the 95\% confidence interval for the slope parameter of the median regression. Also indicate whether there is evidence that its value is null.

#confidence interval for median regressor estimate
area_rqrents%>%tidy()%>%filter(term=='area', tau==0.5)

#no evidence for nullity
```

```{r}
#Produce some plots which explore whether the scale or shape of the rent distribution change as a function of area. 
summary(rents$rent)
index_arearent<-area_rqrents%>%
  augment(newdata=expand_grid(area=seq(20,160, by=1), rent=seq(51,1844, by=10)))%>%
  mutate(.tau=paste0('tau',.tau))%>%
  pivot_wider(names_from = .tau, values_from = .fitted)%>%
  mutate(QSC025=tau0.75-tau0.25,
         QSC01=tau0.9-tau0.1,
         QSK025=(tau0.75-tau0.5)/(tau0.5-tau0.25)-1,
         QSK01=(tau0.9-tau0.5)/(tau0.5-tau0.1)-1)

#plot QSC(0.25)
ggplot(index_arearent)+
  geom_line(aes(area, QSC025))

#plot QSC(0.1)
ggplot(index_arearent)+
  geom_line(aes(area, QSC01))

#plot QSK(0.25)
ggplot(index_arearent)+
  geom_line(aes(area, QSK025))+
  geom_hline(yintercept = 0, col='red')

#plot QSK(0.1)
ggplot(index_arearent)+
  geom_line(aes(area, QSK01))+
  geom_hline(yintercept = 0, col='red')
```


```{r}
#2 model rnetsqm~area
#display data model rentsqm~area
ggplot()+
  geom_point(data=rents, aes(area, rentsqm), col='grey')+
  geom_line(data=area_rqrentsqm%>%augment(), aes(area, .fitted, group = .tau), col='green')
```

3. Use the `incomeEx` dataset. What is the effect of Age on Income? Is this the same for all quantiles of the distribution? Construct models in which the relationship is allowed to be different depending of the race and education level of the household head. Provide a graphical display which presents the estimated models. Is there enough evidence to indicate that the effect of Age on Income is different for different races/education level? Specify the statistical test that would be carried out. 
```{r}
incomeEx <- read.table('C:/Users/anton/OneDrive/SKOOL/R/BUSINESS ANALYTICS/DATA/incomeSurvey.csv', header=TRUE,sep=",")
```


4. (to be done with pen and paper) Draw the QSK(0.1) and QSC(0.25) function of distributions in which the scale is and skewness increase linearly with a predictor X. Next draw the fitted quantile lines for the (0.1,0.25,0.5,0.75,0.9) quantile of a quantile regression model which would lead to such increases in QSK(0.1) and QSC(0.25). Combine different scenarios with strong or weak increases and decreases of the QSC(0.25) and QSK(0.1) functions. 

5. Use the SIPP data: assess how income depends on education at different quantiles (for example 0.1, 0.5 and 0.9): is the effect of education the same for all quantiles? Include the effect of living in a rural setting, both as an additive term as in interaction with education: show the relationship between the different variables (i.e. reproduce the Figures in slide 166 and 167): should one include the rural setting in the estimation? Is the effect comparable for the different quantiles?  


